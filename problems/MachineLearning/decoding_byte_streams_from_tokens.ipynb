{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Byte-level Byte-Pair Encoding (BBPE) is a byte-level variant of Byte-Pair Encoding (BPE). Despite the word 'byte' in its name, BPE does not actually decompose each word into bytes. It instead, breaks down the words into characters. A large drawback however is that if the characters are represented using UTF-8, then there are 130,000 possible Unicode representations. However, in a typical corpus, it may be that only 30,000-60,000 Unicode characters are actually present. Therefore, a base vocabulary set of all 130,000 Unicode characters is memory expensive, and using a smaller base set removes the ability to generalize to characters outside of this base set.\n",
    "\n",
    "BBPE tackles this by going a level further, and decomposing characters into bytes, e.g., a character represented by 3 bytes is separated into 3 bytes and added to the vocabulary. Additionally, the initial vocabulary set is only 256, ($2^4 = 256$, the number of combinations of a single byte). This allows any Unicode to be recreated as it is a sequence of one to four bytes. However, there are only 130,000 Unicode assignments, far less that what can be represented by $2^{32}$ bits. Further, once broken into bytes, the BPE algorithm will merge them into tokens, but it may be that a character needs to be represented by multiple tokens. For example, suppose a waving hand emoji is represented by four bytes, `[B1], [B2], [B3], [B4]`. After running BBPE, we may have merged `[B1], [B2], [B3] -> [B123]`, but `[B4]` is left alone. This might be okay because after training, it the LLM may place the sequence of tokens, `[B123]` before `[B4]` with high probability, however, in rare scenarios it may pair tokens `[B123], [C_1]` where `[C_1]` is some other token, and this combination does not produce a valid Unicode character.\n",
    "\n",
    "Since tokens are ultimately sequences of bytes, it is not necessarily the case that the output of an auto-regressive NLP model will be tokens that are immediately convertible to Unicode characters. Instead, one convert the output stream of tokens into a byte-stream, and thereafter, use a recursive approach to find the optimal grouping of bytes that lends the most reconstructible Unicode characters. This notebook will explore algorithm 1 from [\"Wang et al.\"](https://arxiv.org/abs/1909.03341) which does exactly that.\n",
    "\n",
    "For a given byte sequence $\\left\\{ B \\right\\}_{k=1}^N$, we denote the maximum number of characters that we can recover from it as $f(k)$. Then $f(k)$ has optimal substructure and can be solved by dynamic programming:\n",
    "$$\n",
    "f(k) = \\max_{t=1,2,3,4} \\left\\{ f(k - t) + g(k-t+1, k) \\right\\}\n",
    "$$\n",
    "where $g(i,j)=1$ if $\\{B\\}_{k=i}^j$ corresponds to a valid character, otherwise 0. In addition to solving for $k$, the selections at position $k$ are recording so that the solution may be recovered via back-tracking. $t$ belongs to the set of numbers $\\{1, 2, 3, 4\\}$ because a unicode character may be composed of 1 to 4 bytes. The design of UTF-8 encoding also allows for a unique recovery process which ensures that for a character with multiple bytes, its trailing bytes **will not** make a valid UTF-8 encoded character. For example, if our byte sequence is four bytes, and these four bytes together represent a Unicode character, then $f(4) = 1$, and cannot be larger. In other words, none of the individual bytes, pairings, or triplets, will produce a valid Unicode character. Only the four bytes together will produce the Unicode character."
   ],
   "id": "143677a0e1ff0dac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "UTF-8 has precise bit-pattern rules:\n",
    "\n",
    "| Sequence length |                              Pattern |\n",
    "|:----------------|-------------------------------------:|\n",
    "| 1 byte          |                             0xxxxxxx |\n",
    "| 2 bytes         |                    110xxxxx 10xxxxxx |\n",
    "| 3 bytes         |           1110xxxx 10xxxxxx 10xxxxxx |\n",
    "| 4 bytes         |  11110xxx 10xxxxxx 10xxxxxx 10xxxxxx |\n",
    "\n",
    "This means that it is possible to manually determine if a list of bytes represents a Unicode character in a deterministic fashion."
   ],
   "id": "8edc307bc9dffb0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T00:23:00.759161393Z",
     "start_time": "2025-11-15T00:23:00.714896363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import *\n",
    "from transformers import GPT2Tokenizer\n",
    "import random"
   ],
   "id": "ddc8a9c2e3089616",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-15T00:23:00.816574397Z",
     "start_time": "2025-11-15T00:23:00.759745112Z"
    }
   },
   "source": [
    "def is_valid_pattern_utf8(b: List[int]):\n",
    "    \"\"\"\n",
    "    UTF-8 has precise bit-pattern rules:\n",
    "\n",
    "    +-----------------+-----------------------------------------------+\n",
    "    | Sequence length | Pattern                                       |\n",
    "    +=================+===============================================+\n",
    "    | 1 byte          | 0xxxxxxx                                      |\n",
    "    +-----------------+-----------------------------------------------+\n",
    "    | 2 bytes         | 110xxxxx 10xxxxxx                             |\n",
    "    +-----------------+-----------------------------------------------+\n",
    "    | 3 bytes         | 1110xxxx 10xxxxxx 10xxxxxx                    |\n",
    "    +-----------------+-----------------------------------------------+\n",
    "    | 4 bytes         | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx           |\n",
    "    +-----------------+-----------------------------------------------+\n",
    "\n",
    "    This means that it is possible to manually determine if a list of bytes represents a Unicode character in a deterministic fashion. This method does exactly that.\n",
    "\n",
    "\n",
    "    :param b:   List of 1 to 4 bytes representing a potentially valid Unicode character.\n",
    "    :return:    Boolean indicating if the bytes encoded a valid Unicode character.\n",
    "    \"\"\"\n",
    "    b = list(b)\n",
    "    n = len(b)\n",
    "\n",
    "    assert 1 <= n <= 4, f\"List of bytes must have length in range [1, 4]. Has length {n}.\"\n",
    "\n",
    "    if n == 1:\n",
    "        return b[0] < 0x80\n",
    "\n",
    "    if n == 2:\n",
    "        return (0xC0 <= b[0] <= 0xDF) and (0x80 <= b[1] <= 0xBF)\n",
    "\n",
    "    if n == 3:\n",
    "        return (0xE0 <= b[0] <= 0xEF) and \\\n",
    "               (0x80 <= b[1] <= 0xBF) and \\\n",
    "               (0x80 <= b[2] <= 0xBF)\n",
    "\n",
    "    if n == 4:\n",
    "        return (0xF0 <= b[0] <= 0xF4) and \\\n",
    "               (0x80 <= b[1] <= 0xBF) and \\\n",
    "               (0x80 <= b[2] <= 0xBF) and \\\n",
    "               (0x80 <= b[3] <= 0xBF)\n",
    "\n",
    "    return False\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def is_valid_utf8(b: List[int]) -> bool:\n",
    "    \"\"\"\n",
    "    Fully validates a UTF-8 sequence including:\n",
    "    - Correct leading-byte patterns\n",
    "    - Correct continuation-byte patterns\n",
    "    - No overlong encodings\n",
    "    - No surrogate code points\n",
    "    - No code points > U+10FFFF\n",
    "    \"\"\"\n",
    "\n",
    "    b = list(b)\n",
    "    n = len(b)\n",
    "\n",
    "    if not (1 <= n <= 4):\n",
    "        return False\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) Validate structure (bit patterns)\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # 1-byte (ASCII)\n",
    "    if n == 1:\n",
    "        if b[0] > 0x7F:\n",
    "            return False\n",
    "        codepoint = b[0]\n",
    "\n",
    "    # 2-byte\n",
    "    elif n == 2:\n",
    "        if not (0xC2 <= b[0] <= 0xDF):        # C0/C1 are invalid (overlong)\n",
    "            return False\n",
    "        if not (0x80 <= b[1] <= 0xBF):\n",
    "            return False\n",
    "        codepoint = ((b[0] & 0x1F) << 6) | (b[1] & 0x3F)\n",
    "\n",
    "    # 3-byte\n",
    "    elif n == 3:\n",
    "        b0 = b[0]\n",
    "        if not (0xE0 <= b0 <= 0xEF):\n",
    "            return False\n",
    "\n",
    "        # E0 must have second byte >= A0 (avoid overlongs)\n",
    "        if b0 == 0xE0 and not (0xA0 <= b[1] <= 0xBF):\n",
    "            return False\n",
    "        # ED must have second byte <= 9F (avoid surrogates)\n",
    "        if b0 == 0xED and not (0x80 <= b[1] <= 0x9F):\n",
    "            return False\n",
    "\n",
    "        if not (0x80 <= b[1] <= 0xBF and 0x80 <= b[2] <= 0xBF):\n",
    "            return False\n",
    "\n",
    "        codepoint = ((b[0] & 0x0F) << 12) | ((b[1] & 0x3F) << 6) | (b[2] & 0x3F)\n",
    "\n",
    "    # 4-byte\n",
    "    elif n == 4:\n",
    "        b0 = b[0]\n",
    "        if not (0xF0 <= b0 <= 0xF4):          # F5â€“F7 disallowed\n",
    "            return False\n",
    "\n",
    "        # F0 must have second byte >= 0x90 (avoid overlong)\n",
    "        if b0 == 0xF0 and not (0x90 <= b[1] <= 0xBF):\n",
    "            return False\n",
    "        # F4 must have second byte <= 0x8F (avoid > U+10FFFF)\n",
    "        if b0 == 0xF4 and not (0x80 <= b[1] <= 0x8F):\n",
    "            return False\n",
    "\n",
    "        if not (0x80 <= b[1] <= 0xBF and 0x80 <= b[2] <= 0xBF and 0x80 <= b[3] <= 0xBF):\n",
    "            return False\n",
    "\n",
    "        codepoint = ((b[0] & 0x07) << 18) | ((b[1] & 0x3F) << 12) | \\\n",
    "                    ((b[2] & 0x3F) << 6) | (b[3] & 0x3F)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) Semantic checks on decoded scalar value\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # UTF-16 surrogate range is forbidden\n",
    "    if 0xD800 <= codepoint <= 0xDFFF:\n",
    "        return False\n",
    "\n",
    "    # Maximum valid Unicode scalar value\n",
    "    if codepoint > 0x10FFFF:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# testing the above method:\n",
    "tests = [\n",
    "    b\"a\",             # valid (ASCII character)\n",
    "    b\"\\xc3\\xa9\",      # valid (Ã©)\n",
    "    b\"\\xf0\\x9f\\x98\\x80\",  # valid (ðŸ˜€)\n",
    "    b\"\\xf0\\x28\\x8c\\xbc\",  # invalid UTF-8\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    is_valid = is_valid_utf8(t)\n",
    "    if is_valid:\n",
    "        decoded_ret = t.decode('utf-8')\n",
    "    else:\n",
    "        decoded_ret = \"'invalid unicode representation'\"\n",
    "    print(f\"{t}, decoded result: {decoded_ret}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a', decoded result: a\n",
      "b'\\xc3\\xa9', decoded result: Ã©\n",
      "b'\\xf0\\x9f\\x98\\x80', decoded result: ðŸ˜€\n",
      "b'\\xf0(\\x8c\\xbc', decoded result: 'invalid unicode representation'\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T00:23:00.867393266Z",
     "start_time": "2025-11-15T00:23:00.819618131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_maximal_string(byte_str: List[int]) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "    :param byte_str:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(byte_str)\n",
    "    solutions = [(0, 1) for _ in range(n)]\n",
    "\n",
    "    def _recurse(k: int) -> int:\n",
    "        \"\"\"\n",
    "\n",
    "        :param k:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if k < 0:\n",
    "            # invalid index position\n",
    "            return 0\n",
    "\n",
    "        if k == 0:\n",
    "            # Return whether the byte at this position is a Unicode character or not.\n",
    "            # The best_t is -1 since we cannot move backwards from this position.\n",
    "            best_solution = int(is_valid_utf8([byte_str[k]]))\n",
    "            solutions[0] = (best_solution, 1)\n",
    "            return best_solution\n",
    "\n",
    "        best_solution = 0\n",
    "        best_t = 1\n",
    "        for t in range(1, 5):\n",
    "\n",
    "            if k - t + 1 < 0:\n",
    "                # this start index is out of range in our byte string\n",
    "                break\n",
    "\n",
    "            # extract the byte sequence of length t and check if it is a valid Unicode\n",
    "            seq = byte_str[k - t + 1 : k + 1]\n",
    "            valid = is_valid_utf8(seq)\n",
    "\n",
    "            if not valid:\n",
    "                # Invalid UTF-8 can only consume 1 byte\n",
    "                if t != 1:\n",
    "                    continue\n",
    "                curr_solution = _recurse(k - 1)\n",
    "            else:\n",
    "                # Valid UTF-8 consuming t bytes\n",
    "                curr_solution = _recurse(k - t) + 1\n",
    "\n",
    "            if curr_solution > best_solution:\n",
    "                best_solution = curr_solution\n",
    "                best_t = t\n",
    "\n",
    "        # store the results for backtracking\n",
    "        solutions[k] = (best_solution, best_t)\n",
    "\n",
    "        return best_solution\n",
    "\n",
    "    # Process our stream of bytes.\n",
    "    _recurse(n - 1)\n",
    "    # Backtrack from the last byte, we will include it even if it is invalid.\n",
    "    bt_k = n - 1\n",
    "\n",
    "    solution_list = [] # stores our valid unicode characters in reverse order\n",
    "    while bt_k > -1:\n",
    "        # unpack the best value of t for this subproblem\n",
    "        _, curr_t = solutions[bt_k]\n",
    "        # use this value to get the current byte sequence at this subproblem\n",
    "        byte_seq = byte_str[bt_k - curr_t + 1:bt_k + 1]\n",
    "        if is_valid_utf8(byte_seq):\n",
    "            # This is a valid sequence of bytes we can decode into a UTF-8 character.\n",
    "            curr_char = bytes(byte_seq).decode('utf-8')\n",
    "        else:\n",
    "            # Our optimal sequence has an invalid Unicode character, display î ‹ by default.\n",
    "            curr_char = \"\\uFFFD\"\n",
    "        # append our character\n",
    "        solution_list.append(curr_char)\n",
    "        # advance backwards with respect to our best value of t\n",
    "        bt_k -= curr_t\n",
    "\n",
    "    return \"\".join(solution_list[::-1])"
   ],
   "id": "5ff99b482ab7b23",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T00:23:00.935309244Z",
     "start_time": "2025-11-15T00:23:00.868505210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test our above recursive method\n",
    "subset_bytes = [\n",
    "    b\"a\",                 # valid ASCII\n",
    "    b\"\\xc3\\xa9\",          # valid UTF-8 for \"Ã©\"\n",
    "    b\"\\xf0\\x9f\\x98\\x80\",  # valid UTF-8 for ðŸ˜€\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers to generate UTF-8-style bytes\n",
    "# -------------------------------\n",
    "\n",
    "def gen_leading_byte(choices: Tuple[int] = (1, 2, 3, 4)):\n",
    "    \"\"\"\n",
    "    Generate a UTF-8 leading byte (0xxxxxxx, 110xxxxx, 1110xxxx, 11110xxx).\n",
    "    Each leading byte expects 0, 1, 2, and 3 continuing bytes, respectively.\n",
    "    \"\"\"\n",
    "    choice = random.choice(choices)\n",
    "    if choice == 1:\n",
    "        return random.randrange(0x00, 0x80)          # 0xxxxxxx\n",
    "    elif choice == 2:\n",
    "        return random.randrange(0xC0, 0xE0)          # 110xxxxx\n",
    "    elif choice == 3:\n",
    "        return random.randrange(0xE0, 0xF0)          # 1110xxxx\n",
    "    else:\n",
    "        return random.randrange(0xF0, 0xF8)          # 11110xxx\n",
    "\n",
    "def gen_cont_byte():\n",
    "    \"\"\"\n",
    "    Generate a continuation byte (10xxxxxx).\n",
    "    Such continuation bytes are expected to follow after a leading byte in the\n",
    "    Unicode standard.\n",
    "    \"\"\"\n",
    "    return random.randrange(0x80, 0xC0)\n",
    "\n",
    "def has_valid_proper_prefix(seq: bytes) -> bool:\n",
    "    \"\"\"Return True if any proper prefix of seq is a valid UTF-8 sequence.\"\"\"\n",
    "    for L in range(1, len(seq)):\n",
    "        if is_valid_utf8(list(seq[:L])):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def gen_invalid_utf8(max_attempts: int = 100):\n",
    "    \"\"\"\n",
    "    Generate a sequence of length 1â€“4 where:\n",
    "\n",
    "    - Each byte follows UTF-8 structural rules.\n",
    "    - The sequence is globally an invalid UTF-8. We ensure this by appending the wrong number of continuation bytes.\n",
    "    \"\"\"\n",
    "    for _ in range(max_attempts):\n",
    "        length = random.randint(1, 4)\n",
    "\n",
    "        # Case 1: length == 1 â†’ generate a continuation byte, which is invalid alone\n",
    "        if length == 1:\n",
    "            return bytes([gen_leading_byte((3,))]), 1\n",
    "            # return bytes([gen_cont_byte()])  # e.g. b'\\x90'\n",
    "\n",
    "        # Case 2: invalid because the number of continuation bytes is wrong\n",
    "        lead = gen_leading_byte()\n",
    "\n",
    "        # Determine expected continuation bytes\n",
    "        if 0xC0 <= lead <= 0xDF:   # 110xxxxx â†’ expects 1 continuation byte\n",
    "            expected = 1\n",
    "        elif 0xE0 <= lead <= 0xEF: # 1110xxxx â†’ expects 2\n",
    "            expected = 2\n",
    "        else:                      # 11110xxx â†’ expects 3\n",
    "            expected = 3\n",
    "\n",
    "        # Corrupt the Unicode by generating the wrong number of continuation bytes that the\n",
    "        # leading byte expects. The total length will be in the range 1 through 4.\n",
    "        num_cont_bytes = (expected % 3) + 1\n",
    "        gen_list = [lead] + [gen_cont_byte() for _ in range(num_cont_bytes)]\n",
    "        seq = bytes(gen_list)\n",
    "        if is_valid_utf8(seq) or has_valid_proper_prefix(seq):\n",
    "            continue\n",
    "        return seq, len(gen_list)\n",
    "\n",
    "    return bytes([gen_cont_byte()]), 1\n",
    "\n",
    "def approx_random_bytes(subset, n):\n",
    "    result = b\"\"\n",
    "    labels = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # Choose between:\n",
    "        #  - valid UTF-8 from provided subset\n",
    "        #  - invalid UTF-8 sequence\n",
    "        if random.random() < 0.9:\n",
    "            # Case 1: choose valid char\n",
    "            curr_choice = random.choice(subset)\n",
    "            result += curr_choice\n",
    "            labels.append(curr_choice.decode(\"utf-8\"))\n",
    "            # labels[i] = curr_choice.decode(\"utf-8\")\n",
    "        else:\n",
    "            # Case 2: generate invalid UTF-8\n",
    "            curr_choice, num_bytes = gen_invalid_utf8()\n",
    "            result += curr_choice\n",
    "            labels.extend([\"\\uFFFD\"]*num_bytes)\n",
    "            # labels[i] = \"\\uFFFD\"\n",
    "\n",
    "    return result, \"\".join(labels)\n",
    "\n",
    "k = 5 # generate k random samples of bytes with length n\n",
    "n = 5 # number of choices in each sequence\n",
    "tests = [\n",
    "    approx_random_bytes(subset_bytes, n) for _ in range(k)\n",
    "]\n",
    "tests.append(\n",
    "    # test case where the last byte is invalid\n",
    "    (b'a\\xf0\\x9f\\x98\\x80\\xc3\\xa9a\\xe1', \"aðŸ˜€Ã©aï¿½\")\n",
    ")\n",
    "for test, label in tests:\n",
    "    result = get_maximal_string(test)\n",
    "    test_passed = result == label\n",
    "    print(f\"{test}, \\nground_truth label: {label}\\ndecoded result: {result}\")\n",
    "    print(f\"test {'passed' if test_passed else 'failed'}\\n\")\n"
   ],
   "id": "ee241a1a940d43ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xf0\\x9f\\x98\\x80\\xf0\\x9f\\x98\\x80\\xc3\\xa9aa', \n",
      "ground_truth label: ðŸ˜€ðŸ˜€Ã©aa\n",
      "decoded result: ðŸ˜€ðŸ˜€Ã©aa\n",
      "test passed\n",
      "\n",
      "b'\\xf0\\x9f\\x98\\x80\\xea\\xc3\\xa9aa', \n",
      "ground_truth label: ðŸ˜€ï¿½Ã©aa\n",
      "decoded result: ðŸ˜€ï¿½Ã©aa\n",
      "test passed\n",
      "\n",
      "b'\\xf0\\x9f\\x98\\x80a\\xf0\\x9f\\x98\\x80\\xf1\\xa3a', \n",
      "ground_truth label: ðŸ˜€aðŸ˜€ï¿½ï¿½a\n",
      "decoded result: ðŸ˜€aðŸ˜€ï¿½ï¿½a\n",
      "test passed\n",
      "\n",
      "b'aa\\xc3\\xa9\\xc3\\xa9a', \n",
      "ground_truth label: aaÃ©Ã©a\n",
      "decoded result: aaÃ©Ã©a\n",
      "test passed\n",
      "\n",
      "b'\\xf0\\x9f\\x98\\x80a\\xf0\\x9f\\x98\\x80aa', \n",
      "ground_truth label: ðŸ˜€aðŸ˜€aa\n",
      "decoded result: ðŸ˜€aðŸ˜€aa\n",
      "test passed\n",
      "\n",
      "b'a\\xf0\\x9f\\x98\\x80\\xc3\\xa9a\\xe1', \n",
      "ground_truth label: aðŸ˜€Ã©aï¿½\n",
      "decoded result: aðŸ˜€Ã©aï¿½\n",
      "test passed\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T00:29:51.827314049Z",
     "start_time": "2025-11-15T00:29:51.208183535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"Hello, world! ðŸ˜€\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "d = tokenizer.decode(token_ids)\n",
    "# Convert each token ID to string\n",
    "tokens_as_str = [tokenizer.convert_ids_to_tokens(tid) for tid in token_ids]\n",
    "print(\"\".join(tokens_as_str)) # display the tokens as a string\n",
    "\n",
    "# Convert token strings to bytes\n",
    "tokens_as_bytes = [t.encode('utf-8') for t in tokens_as_str]\n",
    "\n",
    "# Flatten the bytes into a list of integers\n",
    "flat_bytes = [b for token in tokens_as_bytes for b in token]\n",
    "\n",
    "# Reconstruct my byte-level function\n",
    "decoded_text = get_maximal_string(flat_bytes)\n",
    "print(decoded_text)  # should match string as above"
   ],
   "id": "c956a0067fe9fa57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,Ä world!Ä Ã°ÅÄºÄ¢\n",
      "Hello,Ä world!Ä Ã°ÅÄºÄ¢\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
